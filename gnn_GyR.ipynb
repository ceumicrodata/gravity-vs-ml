{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import DCRNN\n",
    "from torch_geometric_temporal.signal import (\n",
    "    temporal_signal_split,\n",
    "    StaticGraphTemporalSignal,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and process email network data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('aves-wildbird-network.csv', delim_whitespace=True, header=None, names=['source', 'target', 'weight', 'timestamp'])\n",
    "\n",
    "aggregates = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create node indices\n",
    "translator = dict(zip(a:=pd.concat([aggregates.source, aggregates.target]).unique(), range(len(a))))\n",
    "aggregates['id_source'] = aggregates.source.map(lambda x: translator[x])\n",
    "aggregates['id_target'] = aggregates.target.map(lambda x: translator[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate network\n",
    "network = nx.from_pandas_edgelist(aggregates, source='id_source', target='id_target', edge_attr='weight', create_using=nx.DiGraph)\n",
    "\n",
    "# Transform to line graph\n",
    "line_graph = nx.line_graph(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final node indices\n",
    "line_node_list = list(line_graph.nodes())\n",
    "line_translator = dict(zip(a:=list(set(line_node_list)), range(len(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create edge list and edge weights\n",
    "# Edge weights are average number of emails of connecting node\n",
    "\n",
    "edges = [[],[]]\n",
    "edge_weights = []\n",
    "for source, target in list(line_graph.edges):\n",
    "    edges[0].append(line_translator[source])\n",
    "    edges[1].append(line_translator[target])\n",
    "    edge_weights.append(aggregates[((aggregates.id_source==source[1])|(aggregates.id_target==source[1]))].weight.mean())\n",
    "\n",
    "edges = np.array(edges)\n",
    "edge_weights = np.array(edge_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data for GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate target arrays\n",
    "numdays = aggregates.timestamp.nunique()\n",
    "aggregates = aggregates.set_index(['id_source', 'id_target', 'timestamp'])\n",
    "line_retranslator = {k:v for v, k in line_translator.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "for day in range(numdays):\n",
    "    daily_targets = []\n",
    "    for node in range(len(line_node_list)):\n",
    "        source, target = line_retranslator[node]\n",
    "        try:\n",
    "            daily_targets.append(aggregates.loc[(source, target, day)].weight)\n",
    "        except KeyError:\n",
    "            daily_targets.append(0)\n",
    "    targets.append(daily_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features (messages in last 10 days)\n",
    "feat_num = 3\n",
    "features = []\n",
    "\n",
    "for day in range(numdays-feat_num):\n",
    "    daily_features = []\n",
    "    for node in range(len(line_node_list)):\n",
    "        node_feat = []\n",
    "        for feature_i in range(feat_num):\n",
    "            node_feat.append(targets[day+feature_i][node])\n",
    "        daily_features.append(node_feat)\n",
    "    features.append(daily_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To array\n",
    "targets = np.array(targets[feat_num:])\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data iterator\n",
    "data = StaticGraphTemporalSignal(edges, edge_weights, features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4658, 3)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dcrnn model\n",
    "\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    \"\"\"Class for a pytorch neural network module\"\"\"\n",
    "\n",
    "    def __init__(self, node_features: int, out_channels: int, filter_size: int):\n",
    "        \"\"\"\n",
    "        Initialize a pytorch model with DCRNN architecture\n",
    "\n",
    "        Args:\n",
    "            node_features:\n",
    "            Number of node features to use.\n",
    "            out_channels:\n",
    "            Number of DCRNN hidden features.\n",
    "            filter_size:\n",
    "            DCRNN filter size.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.recurrent = DCRNN(node_features, out_channels, filter_size)\n",
    "        self.linear = torch.nn.Linear(out_channels, filter_size)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, edge_index: torch.Tensor, edge_weight: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Perform a forward feed\n",
    "        Args:\n",
    "            x:\n",
    "            feature Pytorch Float Tensor\n",
    "            edge_index:\n",
    "            Pytorch Float Tensor of edge indices\n",
    "            edge_weight:\n",
    "            Pytorch Float Tensor of edge weights\n",
    "\n",
    "        Returns:\n",
    "            tens:\n",
    "            Pytorch Float Tensor of Hidden state matrix for all nodes\n",
    "        \"\"\"\n",
    "        tens = self.recurrent(x, edge_index, edge_weight)\n",
    "        tens = F.relu(tens)\n",
    "        tens = self.linear(tens)\n",
    "        return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a model\n",
    "model = RecurrentGCN(3, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization with temporal backpropagation: 100%|██████████| 50/50 [05:12<00:00,  6.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# train with adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "model.train()\n",
    "\n",
    "for _ in tqdm(range(50), \"Optimization with temporal backpropagation\"):\n",
    "    cost = 0\n",
    "    datapoints = 0\n",
    "    for _, snapshot in enumerate(data):\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        cost = cost + torch.mean((y_hat - snapshot.y) ** 2)\n",
    "        datapoints += 1\n",
    "    cost = cost / (datapoints)\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post mortem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0035\n"
     ]
    }
   ],
   "source": [
    "# get predictions\n",
    "yhats = []\n",
    "model.eval()\n",
    "cost = 0\n",
    "for time, snapshot in enumerate(data):\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        yhats.append(y_hat)\n",
    "        cost = cost + torch.mean((y_hat-snapshot.y)**2)\n",
    "cost = cost / (time+1)\n",
    "cost = cost.item()\n",
    "print(\"MSE: {:.4f}\".format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0285],\n",
       "        [0.0285],\n",
       "        [0.0285],\n",
       "        ...,\n",
       "        [0.0285],\n",
       "        [0.0285],\n",
       "        [0.0285]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first period prediction\n",
    "yhats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.02847047], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Check value count of predictions\n",
    "y, idx, count =tf.unique_with_counts(\n",
    "    yhats[0].flatten().detach().numpy(),\n",
    "    out_idx=tf.dtypes.int32,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0059, 0.0076, 0.0064,  ..., 0.0000, 0.0200, 0.0000])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check first period fact\n",
    "enumerate(data).__next__()[1].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bac1c141dd677d21aa3d7e112522370869da97ef62e316f915c40dc907532f18"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('3.10.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e236d32e08e81f213d24544f3b539cfd697b78c2f69d9e99900d1c694707da54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
