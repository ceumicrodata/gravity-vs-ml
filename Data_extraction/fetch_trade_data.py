# This code extracts and saves trade data from https://comtrade.un.org/
import pandas as pd
import requests
import json
import pickle
from collections import defaultdict
from datetime import datetime
from dateutil.relativedelta import relativedelta
import time
from retrying import retry
import tqdm
import os
from dotenv import load_dotenv, find_dotenv

_ = load_dotenv(find_dotenv()) # read local .env file
api_key  = os.environ['COMTRADE_API_KEY']

# Get country codes
hdr ={
    # Request headers
    'Cache-Control': 'no-cache',
    'Ocp-Apim-Subscription-Key': api_key,
    }

country_codes = requests.get('https://comtradeapi.un.org/files/v1/app/reference/Reporters.json')
country_codes = country_codes.text.encode().decode('utf-8-sig') 
country_codes = json.loads(country_codes)["results"]

# Get yearly data availability for all countries
# typeCode: C (commodity)
# freqCode: A (annual)
# clCode: HS (Trade data classification scheme. HS Harmonized System)

data_availability = requests.get('https://comtradeapi.un.org/data/v1/getDa/C/A/HS', headers=hdr)

country_availability = defaultdict(list)

for i in data_availability.json()['data']:
    country_availability[i['reporterCode']].append(datetime.strptime(str(i['period']), '%Y').date())

# Define datelist that will be the timeframe for our analysis
startDate = '1995'
endDate = '2019'

start = datetime.strptime(startDate, '%Y').date()
end = datetime.strptime(endDate, '%Y').date()

datelist = []

while start <= end:
    datelist.append(start)
    start += relativedelta(years=1)

# Define how many years can be missing from a given country in the timeframe
# Obtain list of countries to which we have sufficient data
max_missing_years = 0
countries_with_sufficient_data = []

for country, dates in country_availability.items():
    missing_years = 0
    for j in datelist:
        if j not in dates:
            missing_years+=1
    if missing_years<=max_missing_years:
        countries_with_sufficient_data.append(str(country))

#Create date list in batches of 5
datelist_string = [str(i.strftime('%Y')) for i in datelist]
date_string_in_fives = []
for i in range(int(len(datelist_string)/5)+1):
    date_string_in_fives.append(",".join(datelist_string[i*5:i*5+5]))

#Export countries in batches of 5 with new API
countries_string_in_fives = []
for i in range(int(len(countries_with_sufficient_data)/5)+1):
    countries_string_in_fives.append(",".join(countries_with_sufficient_data[i*5:i*5+5]))

def retry_if_connection_error(exception):
    return isinstance(exception, ConnectionError)

def retry_if_status_code_not_200(result):
    return result.status_code!=200

# if exception retry with 2 second wait  
@retry(retry_on_exception=retry_if_connection_error, retry_on_result=retry_if_status_code_not_200, wait_fixed=2000)
def safe_request(url, **kwargs):
    return requests.get(url, **kwargs)

exported_data_list = []
for element1 in countries_string_in_fives:
    for element2 in tqdm.tqdm(countries_string_in_fives):
        for date in date_string_in_fives:
            exported_data = safe_request(f'https://comtradeapi.un.org/data/v1/get/C/A/HS?reporterCode={element1}&period={date}&partnerCode={element2}&cmdCode=total&flowCode=M&breakdownMode=classic', headers=hdr)
            exported_data_list.append(exported_data)
            time.sleep(4)

#with open('exported_data_list_new_annual_import.pickle', 'wb') as handle:
#    pickle.dump(exported_data_list, handle, protocol=pickle.HIGHEST_PROTOCOL)

#At the most basic data record, up to three trade values could be presented according to following rules:
#a) CIF value for imports would be assigned to “CIF Value”
#b) When FOB value is reported for imports, it would be allocated to “FOB Value”
#c) FOB value for exports would be allocated to “FOB Value”
#For users’ convenience, a third (“primary”) trade value is generated by applying IMTS 2010 recommendation on valuation: if available CIF otherwise FOB for imports; FOB for exports, respectively.

final_values = []
final_values_full = []

for export in exported_data_list:
    exported_data = export.json()['data']
    for entry in exported_data:
        final_values.append([entry['reporterCode'], entry['partnerCode'], entry['period'], entry['primaryValue']])
        final_values_full.append(entry)

final_df = pd.DataFrame(final_values, columns=['Reporter', 'Partner', 'Period', 'Value'])
final_df_full = pd.DataFrame(final_values_full)

#final_df.to_csv('trade_data_new_annual_import.csv', index=False)
#final_df_full.to_csv('trade_data_new_annual_import_full_columns.csv', index=False)

# Add zero flows
country_codes = set(final_df["Reporter"]) | set(final_df["Partner"])
year_dates = set(final_df["Period"])
available_data = set(zip(final_df.Reporter, final_df.Partner, final_df.Period))
all_data = set([(origin, destination, time_period) for origin in country_codes for destination in country_codes for time_period in year_dates])
zero_flow_data = list(all_data - available_data)

zero_flow_df = pd.DataFrame(zero_flow_data, columns =['Reporter', 'Partner', 'Period'])
zero_flow_df['Value'] = 0

final_df = pd.concat([final_df, zero_flow_df])
final_df = final_df.sort_values(by=['Reporter', 'Partner', 'Period'])

final_df['Value'] = final_df['Value'].astype('int64')

final_df.to_csv('../Input_datasets/Yearly_trade_data_prediction/trade_data_new_annual_import_zero_padded.csv', index=False)

# Export country names that have annual trade data
country_codes_dict = {}

for element in country_codes:
    country_codes_dict[element['id']] = element['text']

country_names_to_work_with = []
for country in countries_with_sufficient_data:
    country_names_to_work_with.append([country, country_codes_dict[int(country)]])

country_names_to_work_with_df = pd.DataFrame(country_names_to_work_with, columns=['ISO_3166-1_numeric_code', 'Country_name'])
country_names_to_work_with_df.to_csv('../Input_datasets/Yearly_trade_data_prediction/country_names_with_annual_trade_data.csv')