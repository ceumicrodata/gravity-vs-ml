{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebbook merges and validates the datasets used for trade modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import requests\n",
    "import json\n",
    "import codecs\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Import datasets\n",
    "###############\n",
    "\n",
    "# Country name mapping table\n",
    "country_mapping_table = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/country_codes.csv')\n",
    "iso_alpha3_numeric_mapper = dict(zip(country_mapping_table['ISO3166-1-Alpha-3'], country_mapping_table['ISO3166-1-numeric']))\n",
    "iso_alpha2_numeric_mapper = dict(zip(country_mapping_table['ISO3166-1-Alpha-2'], country_mapping_table['ISO3166-1-numeric']))\n",
    "\n",
    "# Trade dataset\n",
    "trade_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/trade_data_new_annual_import_zero_padded.csv')\n",
    "country_names_in_trade_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/country_names_with_annual_trade_data.csv')\n",
    "trade_dataset['Period'] = trade_dataset['Period'].astype(int)\n",
    "\n",
    "# Additional edge characteristics\n",
    "cepii_edge_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/cepii_edge.csv')\n",
    "# Drop countries that cannot be mapped\n",
    "cepii_edge_dataset = cepii_edge_dataset[(cepii_edge_dataset['iso_o']!='ANT') & (cepii_edge_dataset['iso_d']!='ANT')\n",
    "                                        & (cepii_edge_dataset['iso_o']!='PAL') & (cepii_edge_dataset['iso_d']!='PAL')\n",
    "                                        & (cepii_edge_dataset['iso_o']!='TMP') & (cepii_edge_dataset['iso_d']!='TMP')\n",
    "                                        & (cepii_edge_dataset['iso_o']!='YUG') & (cepii_edge_dataset['iso_d']!='YUG')\n",
    "                                        & (cepii_edge_dataset['iso_o']!='ZAR') & (cepii_edge_dataset['iso_d']!='ZAR')]\n",
    "cepii_edge_dataset['iso_o'] = cepii_edge_dataset['iso_o'].apply(lambda x: x if x!='ROM' else 'ROU')\n",
    "cepii_edge_dataset['iso_d'] = cepii_edge_dataset['iso_d'].apply(lambda x: x if x!='ROM' else 'ROU')\n",
    "\n",
    "cepii_edge_dataset['iso_o'] = cepii_edge_dataset['iso_o'].astype('string').apply(lambda x: iso_alpha3_numeric_mapper[x])\n",
    "cepii_edge_dataset['iso_d'] = cepii_edge_dataset['iso_d'].astype('string').apply(lambda x: iso_alpha3_numeric_mapper[x])\n",
    "\n",
    "# Node characteristics\n",
    "wbg_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/WBG_data_all_countries.csv')\n",
    "wbg_dataset['economy'] = wbg_dataset['economy'].astype('string').apply(lambda x: iso_alpha3_numeric_mapper[x])\n",
    "wbg_dataset.columns = ['economy', 'year', 'gdp', 'total_population', 'urban_population(%_of_total)']\n",
    "wbg_dataset['year'] = wbg_dataset['year'].apply(lambda x: x[2:])\n",
    "wbg_dataset['year'] = wbg_dataset['year'].astype(int)\n",
    "\n",
    "country_groups_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/country_groups.csv')\n",
    "# Drop countries that cannot be mapped\n",
    "country_groups_dataset = country_groups_dataset[(country_groups_dataset['country_code']!='JA')]\n",
    "country_groups_dataset['country_code'] = country_groups_dataset['country_code'].apply(lambda x: iso_alpha2_numeric_mapper[x])\n",
    "\n",
    "cepii_nodes_dataset = pd.read_csv('../Input_datasets/Yearly_trade_data_prediction/cepii_node.csv')\n",
    "# Drop countries that cannot be mapped\n",
    "cepii_nodes_dataset = cepii_nodes_dataset[(cepii_nodes_dataset['iso3']!='ANT')\n",
    "                                        & (cepii_nodes_dataset['iso3']!='PAL') \n",
    "                                        & (cepii_nodes_dataset['iso3']!='TMP')\n",
    "                                        & (cepii_nodes_dataset['iso3']!='YUG')\n",
    "                                        & (cepii_nodes_dataset['iso3']!='ZAR')\n",
    "                                        ]\n",
    "cepii_nodes_dataset['iso3'] = cepii_nodes_dataset['iso3'].apply(lambda x: x if x!='ROM' else 'ROU')\n",
    "cepii_nodes_dataset['iso_3'] = cepii_nodes_dataset['iso3']\n",
    "cepii_nodes_dataset['iso3'] = cepii_nodes_dataset['iso3'].apply(lambda x: iso_alpha3_numeric_mapper[x])\n",
    "\n",
    "# Drop city info as it causes duplicates\n",
    "cepii_nodes_dataset.drop(columns=[\"city_en\", \"city_fr\", \"lat\", \"lon\", \"cap\", \"maincity\"], inplace=True)\n",
    "cepii_nodes_dataset.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Validate trade data\n",
    "###############\n",
    "\n",
    "# Obtain list of countries from trade dataset and validate\n",
    "\n",
    "reporter_set = set(trade_dataset.Reporter.unique())\n",
    "partner_set = set(trade_dataset.Partner.unique())\n",
    "\n",
    "if (reporter_set - partner_set != set()) & (partner_set - reporter_set != set()):\n",
    "    print('Number of partners and reporters do no match!')\n",
    "\n",
    "periods = set(trade_dataset.Period.unique())\n",
    "all_pairs = set([(i,j,k) for i in reporter_set for j in partner_set for k in periods])\n",
    "real_pairs = set(list(trade_dataset[['Reporter', 'Partner', 'Period']].itertuples(index=False, name=None)))\n",
    "\n",
    "if (all_pairs - real_pairs != set()) & (real_pairs - all_pairs != set()):\n",
    "    print('Number of expected and real observations do no match!')\n",
    "\n",
    "# Clean errorous codes\n",
    "\n",
    "iso_codes_in_country_names_in_trade_dataset = set(country_names_in_trade_dataset['ISO_3166-1_numeric_code'])\n",
    "if (reporter_set - iso_codes_in_country_names_in_trade_dataset != set()) & (iso_codes_in_country_names_in_trade_dataset - reporter_set != set()):\n",
    "    print('ISO codes in trade dataset and country_names_in_trade_dataset do not match!')\n",
    "\n",
    "errorous_country_code_mapper = dict(zip(reporter_set, reporter_set))\n",
    "errorous_country_code_mapper[251] = 250\n",
    "errorous_country_code_mapper[579] = 578\n",
    "errorous_country_code_mapper[699] = 356\n",
    "errorous_country_code_mapper[757] = 756\n",
    "errorous_country_code_mapper[842] = 840\n",
    "\n",
    "trade_dataset['Reporter'] = trade_dataset['Reporter'].apply(lambda x: errorous_country_code_mapper[x])\n",
    "trade_dataset['Partner'] = trade_dataset['Partner'].apply(lambda x: errorous_country_code_mapper[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Merge cepii to trade data\n",
    "###############\n",
    "\n",
    "trade_dataset['Reporter'] = trade_dataset['Reporter'].astype('float64')\n",
    "trade_dataset['Partner'] = trade_dataset['Partner'].astype('float64')\n",
    "\n",
    "trade_edgelist = pd.merge(trade_dataset, cepii_edge_dataset, left_on = ['Reporter', 'Partner'], right_on = ['iso_o','iso_d'], how='left')\n",
    "trade_edgelist['iso_o'] = trade_edgelist['iso_o'].astype('int')\n",
    "trade_edgelist['iso_d'] = trade_edgelist['iso_d'].astype('int')\n",
    "trade_edgelist.drop(columns=['Reporter', 'Partner'], inplace=True)\n",
    "\n",
    "###############\n",
    "# Drop edges where origin = destination\n",
    "###############\n",
    "\n",
    "trade_edgelist = trade_edgelist[trade_edgelist['iso_o'] != trade_edgelist['iso_d']]\n",
    "\n",
    "###############\n",
    "# Filter wbg to trade data countries\n",
    "# Merge with cepii_nodes and country groups\n",
    "###############\n",
    "\n",
    "trade_dataset_countries = trade_dataset['Reporter'].unique()\n",
    "wbg_dataset = wbg_dataset[wbg_dataset['economy'].isin(trade_dataset_countries)]\n",
    "\n",
    "#trade_nodelist = pd.merge(wbg_dataset, country_groups_dataset, left_on = ['economy'], right_on = ['country_code'], how='left')\n",
    "trade_nodelist = pd.merge(wbg_dataset, cepii_nodes_dataset, left_on = ['economy'], right_on = ['iso3'], how='left')\n",
    "trade_nodelist['iso_numeric'] = trade_nodelist['iso3'].astype('int')\n",
    "trade_nodelist.drop(columns=['economy', 'iso3'], inplace=True)\n",
    "\n",
    "###############\n",
    "# Use backfill for gdp data\n",
    "###############\n",
    "trade_nodelist.gdp = trade_nodelist.groupby('country').gdp.bfill()\n",
    "\n",
    "###############\n",
    "# Fill citynum for Macao\n",
    "###############\n",
    "trade_nodelist['citynum'] = trade_nodelist.apply(lambda x: 1 if x['iso_numeric']==446 else x['citynum'], axis=1)\n",
    "\n",
    "###############\n",
    "# Fill distw,distwces for Macao\n",
    "###############\n",
    "trade_edgelist['distw'] = trade_edgelist.apply(lambda x: x['dist'] if (x['iso_o']==446) | (x['iso_d']==446) else x['distw'], axis=1)\n",
    "trade_edgelist['distwces'] = trade_edgelist.apply(lambda x: x['dist'] if (x['iso_o']==446) | (x['iso_d']==446) else x['distwces'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Add reverse flow value\n",
    "###############\n",
    "\n",
    "trade_edgelist_reverse = trade_edgelist[[\"Period\", \"Value\", \"iso_o\", \"iso_d\"]].copy()\n",
    "trade_edgelist_reverse.rename(columns = {\"Value\": \"Value_reverse\", \"iso_o\":\"iso_d\", \"iso_d\":\"iso_o\"}, inplace=True)\n",
    "\n",
    "trade_edgelist = pd.merge(trade_edgelist, trade_edgelist_reverse, on=[\"Period\", \"iso_o\", \"iso_d\"], how=\"left\")\n",
    "\n",
    "###############\n",
    "# All to node\n",
    "###############\n",
    "all_to_node = trade_edgelist.groupby([\"Period\",  \"iso_d\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "all_to_node.rename(columns={\"Value\": \"all_to_d\"}, inplace=True)\n",
    "\n",
    "trade_edgelist = pd.merge(trade_edgelist, all_to_node, on=[\"Period\", \"iso_d\"], how=\"left\")\n",
    "\n",
    "all_to_node.rename(columns={\"all_to_d\": \"all_to_o\", \"iso_d\":\"iso_o\"}, inplace=True)\n",
    "\n",
    "trade_edgelist = pd.merge(trade_edgelist, all_to_node, on=[\"Period\", \"iso_o\"], how=\"left\")\n",
    "\n",
    "###############\n",
    "# Node to all\n",
    "###############\n",
    "node_to_all = trade_edgelist.groupby([\"Period\",  \"iso_o\"])[\"Value\"].sum().reset_index()\n",
    "\n",
    "node_to_all.rename(columns={\"Value\": \"o_to_all\"}, inplace=True)\n",
    "\n",
    "trade_edgelist = pd.merge(trade_edgelist, node_to_all, on=[\"Period\", \"iso_o\"], how=\"left\")\n",
    "\n",
    "node_to_all.rename(columns={\"o_to_all\": \"d_to_all\", \"iso_o\":\"iso_d\"}, inplace=True)\n",
    "\n",
    "trade_edgelist = pd.merge(trade_edgelist, node_to_all, on=[\"Period\", \"iso_d\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Save datasets\n",
    "###############\n",
    "trade_nodelist.to_csv('../Output_datasets/Yearly_trade_data_prediction/trade_nodelist.csv')\n",
    "trade_edgelist.to_csv('../Output_datasets/Yearly_trade_data_prediction/trade_edgelist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node dataset\n",
    "node_id='iso_numeric'\n",
    "node_timestamp='year'\n",
    "node_features=['gdp', 'total_population',\n",
    "               'urban_population(%_of_total)',\n",
    "               'area', 'dis_int', 'landlocked', 'citynum']\n",
    "\n",
    "# Edge dataset\n",
    "flow_origin='iso_o'\n",
    "flow_destination='iso_d'\n",
    "flows_value='Value'\n",
    "flows_timestamp='Period'\n",
    "flows_features=['contig', 'comlang_off', 'comlang_ethno', 'colony',\n",
    "                'comcol', 'curcol', 'col45', 'smctry', 'dist', 'distcap', 'distw', 'distwces',\n",
    "                'Value_reverse', 'all_to_d', 'all_to_o', 'o_to_all', 'd_to_all']\n",
    "\n",
    "# Chunk parameters\n",
    "chunk_size = 5\n",
    "\n",
    "# Chunked data path\n",
    "chunk_path  = \"../Output_datasets/Yearly_trade_data_prediction/Chunked_merged_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rgyuri/.pyenv/versions/3.11.3/lib/python3.11/site-packages/pandas/core/frame.py:5039: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().rename(\n",
      "/var/folders/qt/kwzdyjg14cq25v2k2b2l6mt40000gn/T/ipykernel_1847/3156250326.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trade_nodelist[flow_destination] = trade_nodelist[flow_origin]\n"
     ]
    }
   ],
   "source": [
    "# Filter only neccesary columns\n",
    "nodes_columns = node_features + [node_id] + [node_timestamp]\n",
    "trade_nodelist = trade_nodelist[nodes_columns]\n",
    "\n",
    "edges_columns = flows_features + [flow_origin] + [flow_destination] + \\\n",
    "    [flows_timestamp] + [flows_value]\n",
    "trade_edgelist = trade_edgelist[edges_columns]\n",
    "\n",
    "# Merge nodes and edges\n",
    "trade_nodelist.rename(columns={node_id: flow_origin, node_timestamp: flows_timestamp}, inplace=True)\n",
    "trade_nodelist[flow_destination] = trade_nodelist[flow_origin]\n",
    "\n",
    "nodes_and_edges = pd.merge(pd.merge(trade_edgelist, trade_nodelist.drop(flow_destination, axis=1), how='left', on=[flow_origin, flows_timestamp]),\n",
    "                        trade_nodelist.drop(flow_origin, axis=1), how='left', on=[flow_destination, flows_timestamp], \n",
    "                        suffixes=('_o', '_d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_period_list = [nodes_and_edges[flows_timestamp].unique()[i:i+chunk_size] for i in range(0, len(nodes_and_edges[flows_timestamp].unique())-(chunk_size-1))]\n",
    "for chunk in chunk_period_list[:-1]:\n",
    "    nodes_and_edges_chunk = nodes_and_edges[nodes_and_edges[flows_timestamp].isin(chunk)]\n",
    "    chunk_name = str(min(chunk)) + \"-\" + str(max(chunk))\n",
    "    nodes_and_edges_chunk.to_csv(f\"{chunk_path}/{chunk_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7d815e1f708ebb1f5714e36652b1af8a8532e2538a9a16a8fb7dc9a472af4e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
