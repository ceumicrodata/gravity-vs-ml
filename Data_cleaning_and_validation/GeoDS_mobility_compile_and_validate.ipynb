{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebbook merges and validates the datasets for Google mobility prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Import datasets\n",
    "###############\n",
    "\n",
    "# State name mapping table\n",
    "state_mapping_table = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/us_states_w_fips_and_coordinates.csv')\n",
    "# Drop US-DC (District of Columbia) as it is not a state\n",
    "state_mapping_table = state_mapping_table[state_mapping_table[\"iso_3166_2_code\"]!=\"US-DC\"]\n",
    "iso_2_fips_mapper = dict(zip(state_mapping_table['iso_3166_2_code'], state_mapping_table['FIPS']))\n",
    "fips_iso_2_mapper = dict(zip(state_mapping_table['FIPS'], state_mapping_table['iso_3166_2_code']))\n",
    "\n",
    "# Google Mobility dataset\n",
    "geods_mobility_dataset = pd.read_csv('../Input_datasets/GeoDS_mobility_flow_prediction/state2state_merged.csv')\n",
    "# Drop Puerto Rico and US-DC (District of Columbia)\n",
    "geods_mobility_dataset = geods_mobility_dataset[(geods_mobility_dataset['geoid_o']!=11) & (geods_mobility_dataset['geoid_d']!=11) &\n",
    "                                                (geods_mobility_dataset['geoid_o']!=72) & (geods_mobility_dataset['geoid_d']!=72)]\n",
    "\n",
    "# Import US state edgelist\n",
    "edge_list = pd.read_json('../Input_datasets/Mobility_flow_prediction_shared/us_states_edge_list.json')\n",
    "edge_list.columns= ['origin', 'destination']\n",
    "# Drop FIPS=11 (US-DC, District of Columbia) as it is not a state\n",
    "edge_list = edge_list[(edge_list['origin']!=11) & (edge_list['destination']!=11)]\n",
    "\n",
    "# Additional edge characteristics\n",
    "us_state_distances = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/US_state_distances.csv', skiprows=2, index_col=0)\n",
    "us_state_distances.columns = us_state_distances.index\n",
    "\n",
    "# Node characteristics - US state population\n",
    "us_state_pop = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/US_state_pop_2019_census.csv', index_col=0)\n",
    "us_state_pop.columns = [\"state\", \"population_2019\", \"population_density_2019\", \"FIPS\"]\n",
    "us_state_pop.drop(columns=[\"state\"], inplace=True)\n",
    "\n",
    "# Node characteristics - OpenStreetMap features\n",
    "overpass_features = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/overpass_features.csv', index_col=0)\n",
    "overpass_features.drop(columns=[\"overpass_id\", \"state\"], inplace=True)\n",
    "overpass_features.rename(columns={\"state_short\":\"iso_3166_2_code\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Create node_list data\n",
    "###############\n",
    "node_list = pd.merge(state_mapping_table, us_state_pop, on = \"FIPS\", how='left')\n",
    "node_list = pd.merge(node_list, overpass_features, on=\"iso_3166_2_code\", how='left')\n",
    "\n",
    "###############\n",
    "# Create edge_list data\n",
    "###############\n",
    "edge_list[\"distances\"] = edge_list.apply(lambda x: us_state_distances.loc[x[\"origin\"], x[\"destination\"]], axis=1)\n",
    "edge_list['origin'] = edge_list['origin'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_list['destination'] = edge_list['destination'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "\n",
    "###############\n",
    "# Create edge_target_list data\n",
    "###############\n",
    "edge_target_list = geods_mobility_dataset.copy()\n",
    "edge_target_list['geoid_o'] = edge_target_list['geoid_o'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_target_list['geoid_d'] = edge_target_list['geoid_d'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_target_list.rename(columns={\"geoid_o\":\"origin\", \"geoid_d\": \"destination\"}, inplace=True)\n",
    "\n",
    "###############\n",
    "# Validate mobility data\n",
    "###############\n",
    "\n",
    "# Obtain list of countries from trade dataset and validate\n",
    "\n",
    "origin_set = set(edge_target_list.origin.unique())\n",
    "destination_set = set(edge_target_list.destination.unique())\n",
    "\n",
    "if (origin_set - destination_set != set()) & (destination_set - origin_set != set()):\n",
    "    print('Number of partners and reporters do no match!')\n",
    "\n",
    "periods = set(edge_target_list.start_date.unique())\n",
    "all_pairs = set([(i,j,k) for i in origin_set for j in destination_set for k in periods])\n",
    "real_pairs = set(list(edge_target_list[['origin', 'destination', 'start_date']].itertuples(index=False, name=None)))\n",
    "\n",
    "if (all_pairs - real_pairs != set()) & (real_pairs - all_pairs != set()):\n",
    "    print('Number of expected and real observations do no match!')\n",
    "\n",
    "###############\n",
    "# Save to csv\n",
    "###############\n",
    "node_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/node_list.csv\")\n",
    "edge_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/edge_list.csv\")\n",
    "edge_target_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/edge_target_list.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac1c141dd677d21aa3d7e112522370869da97ef62e316f915c40dc907532f18"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
