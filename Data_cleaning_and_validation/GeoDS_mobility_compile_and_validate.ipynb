{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebbook merges and validates the datasets for GeoDS mobility prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Import datasets\n",
    "###############\n",
    "\n",
    "# State name mapping table\n",
    "state_mapping_table = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/us_states_w_fips_and_coordinates.csv')\n",
    "# Drop US-DC (District of Columbia) as it is not a state\n",
    "state_mapping_table = state_mapping_table[state_mapping_table[\"iso_3166_2_code\"]!=\"US-DC\"]\n",
    "iso_2_fips_mapper = dict(zip(state_mapping_table['iso_3166_2_code'], state_mapping_table['FIPS']))\n",
    "fips_iso_2_mapper = dict(zip(state_mapping_table['FIPS'], state_mapping_table['iso_3166_2_code']))\n",
    "\n",
    "# GeoDS Mobility dataset\n",
    "geods_mobility_dataset = pd.read_csv('../Input_datasets/GeoDS_mobility_flow_prediction/state2state_merged.csv')\n",
    "# Drop Puerto Rico and US-DC (District of Columbia)\n",
    "geods_mobility_dataset = geods_mobility_dataset[(geods_mobility_dataset['geoid_o']!=11) & (geods_mobility_dataset['geoid_d']!=11) &\n",
    "                                                (geods_mobility_dataset['geoid_o']!=72) & (geods_mobility_dataset['geoid_d']!=72)]\n",
    "\n",
    "# Import US state edgelist\n",
    "edge_list = pd.read_json('../Input_datasets/Mobility_flow_prediction_shared/us_states_edge_list.json')\n",
    "edge_list.columns= ['origin', 'destination']\n",
    "# Drop FIPS=11 (US-DC, District of Columbia) as it is not a state\n",
    "edge_list = edge_list[(edge_list['origin']!=11) & (edge_list['destination']!=11)]\n",
    "# Add neighbouring column\n",
    "edge_list['neighbouring'] = 1\n",
    "# Create full edge list\n",
    "full_edge_list = pd.DataFrame({(i, j) for i in edge_list['destination'].unique() for j in edge_list['destination'].unique()}, columns=['origin', 'destination'])\n",
    "edge_list = pd.merge(full_edge_list, edge_list, on=['origin', 'destination'], how='left')\n",
    "# Fill NA for eighbouring column\n",
    "edge_list = edge_list.fillna(0)\n",
    "\n",
    "# Additional edge characteristics\n",
    "us_state_distances = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/US_state_distances.csv', skiprows=2, index_col=0)\n",
    "us_state_distances.columns = us_state_distances.index\n",
    "\n",
    "# Node characteristics - US state population\n",
    "us_state_pop = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/US_state_pop_2019_census.csv', index_col=0)\n",
    "us_state_pop.columns = [\"state\", \"population_2019\", \"population_density_2019\", \"FIPS\"]\n",
    "us_state_pop.drop(columns=[\"state\"], inplace=True)\n",
    "\n",
    "# Node characteristics - OpenStreetMap features\n",
    "overpass_features = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/overpass_features.csv', index_col=0)\n",
    "overpass_features.drop(columns=[\"overpass_id\", \"state\"], inplace=True)\n",
    "overpass_features.rename(columns={\"state_short\":\"iso_3166_2_code\"}, inplace=True)\n",
    "\n",
    "# US state size\n",
    "us_state_size = pd.read_csv('../Input_datasets/Mobility_flow_prediction_shared/US_state_sizes.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Create node_list data\n",
    "###############\n",
    "node_list = pd.merge(state_mapping_table, us_state_pop, on = \"FIPS\", how='left')\n",
    "node_list = pd.merge(node_list, overpass_features, on=\"iso_3166_2_code\", how='left')\n",
    "node_list = pd.merge(node_list, us_state_size, on=\"FIPS\", how='left')\n",
    "\n",
    "###############\n",
    "# Drop Alaska and Hawaii from node list\n",
    "###############\n",
    "node_list = node_list[(node_list['iso_3166_2_code']!='US-AK') &\n",
    "                      (node_list['iso_3166_2_code']!='US-HI')]\n",
    "\n",
    "###############\n",
    "# Create edge_list data\n",
    "###############\n",
    "edge_list[\"distances\"] = edge_list.apply(lambda x: us_state_distances.loc[x[\"origin\"], x[\"destination\"]], axis=1)\n",
    "edge_list['origin'] = edge_list['origin'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_list['destination'] = edge_list['destination'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "\n",
    "###############\n",
    "# Drop edges where origin = destination\n",
    "###############\n",
    "\n",
    "edge_list = edge_list[edge_list['origin'] != edge_list['destination']]\n",
    "\n",
    "###############\n",
    "# Create edge_target_list data\n",
    "###############\n",
    "edge_target_list = geods_mobility_dataset.copy()\n",
    "edge_target_list['geoid_o'] = edge_target_list['geoid_o'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_target_list['geoid_d'] = edge_target_list['geoid_d'].astype('int').apply(lambda x: fips_iso_2_mapper[x])\n",
    "edge_target_list.rename(columns={\"geoid_o\":\"origin\", \"geoid_d\": \"destination\"}, inplace=True)\n",
    "\n",
    "###############\n",
    "# Drop edges where origin = destination\n",
    "###############\n",
    "\n",
    "edge_target_list = edge_target_list[edge_target_list['origin'] != edge_target_list['destination']]\n",
    "\n",
    "###############\n",
    "# Drop Alaska and Hawaii from node target list\n",
    "###############\n",
    "edge_target_list = edge_target_list[(edge_target_list['origin']!='US-AK') &\n",
    "                      (edge_target_list['origin']!='US-HI') &\n",
    "                      (edge_target_list['destination']!='US-AK') &\n",
    "                      (edge_target_list['destination']!='US-HI')]\n",
    "\n",
    "\n",
    "###############\n",
    "# Add timeline column which will be used as timestamp\n",
    "###############\n",
    "timeline_dict = dict(enumerate(sorted(edge_target_list['start_date'].unique())))\n",
    "timeline_mapping_dict = {y: x for x, y in timeline_dict.items()}\n",
    "\n",
    "edge_target_list['Timeline'] = edge_target_list['start_date'].apply(lambda x: timeline_mapping_dict[x])\n",
    "\n",
    "###############\n",
    "# Validate mobility data\n",
    "###############\n",
    "\n",
    "# Obtain list of countries from trade dataset and validate\n",
    "\n",
    "origin_set = set(edge_target_list.origin.unique())\n",
    "destination_set = set(edge_target_list.destination.unique())\n",
    "\n",
    "if (origin_set - destination_set != set()) & (destination_set - origin_set != set()):\n",
    "    print('Number of partners and reporters do no match!')\n",
    "\n",
    "periods = set(edge_target_list.start_date.unique())\n",
    "all_pairs = set([(i,j,k) for i in origin_set for j in destination_set for k in periods])\n",
    "real_pairs = set(list(edge_target_list[['origin', 'destination', 'start_date']].itertuples(index=False, name=None)))\n",
    "\n",
    "if (all_pairs - real_pairs != set()) & (real_pairs - all_pairs != set()):\n",
    "    print('Number of expected and real observations do no match!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "      <th>start_date</th>\n",
       "      <th>visitor_flows</th>\n",
       "      <th>pop_flows</th>\n",
       "      <th>Timeline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>US-AL</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>2019-01-07</td>\n",
       "      <td>1094</td>\n",
       "      <td>14755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>US-AL</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>2019-01-14</td>\n",
       "      <td>1654</td>\n",
       "      <td>22674</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>US-AL</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>2019-01-21</td>\n",
       "      <td>1221</td>\n",
       "      <td>17391</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>US-AL</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>2019-01-28</td>\n",
       "      <td>1279</td>\n",
       "      <td>18609</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>US-AL</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>1122</td>\n",
       "      <td>14979</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413395</th>\n",
       "      <td>US-WY</td>\n",
       "      <td>US-WI</td>\n",
       "      <td>2021-11-29</td>\n",
       "      <td>122</td>\n",
       "      <td>1824</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413396</th>\n",
       "      <td>US-WY</td>\n",
       "      <td>US-WI</td>\n",
       "      <td>2021-12-06</td>\n",
       "      <td>80</td>\n",
       "      <td>1268</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413397</th>\n",
       "      <td>US-WY</td>\n",
       "      <td>US-WI</td>\n",
       "      <td>2021-12-13</td>\n",
       "      <td>160</td>\n",
       "      <td>2577</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413398</th>\n",
       "      <td>US-WY</td>\n",
       "      <td>US-WI</td>\n",
       "      <td>2021-12-20</td>\n",
       "      <td>243</td>\n",
       "      <td>3894</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413399</th>\n",
       "      <td>US-WY</td>\n",
       "      <td>US-WI</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>328</td>\n",
       "      <td>5143</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351936 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       origin destination  start_date  visitor_flows  pop_flows  Timeline\n",
       "312     US-AL       US-AZ  2019-01-07           1094      14755         0\n",
       "313     US-AL       US-AZ  2019-01-14           1654      22674         1\n",
       "314     US-AL       US-AZ  2019-01-21           1221      17391         2\n",
       "315     US-AL       US-AZ  2019-01-28           1279      18609         3\n",
       "316     US-AL       US-AZ  2019-02-04           1122      14979         4\n",
       "...       ...         ...         ...            ...        ...       ...\n",
       "413395  US-WY       US-WI  2021-11-29            122       1824       151\n",
       "413396  US-WY       US-WI  2021-12-06             80       1268       152\n",
       "413397  US-WY       US-WI  2021-12-13            160       2577       153\n",
       "413398  US-WY       US-WI  2021-12-20            243       3894       154\n",
       "413399  US-WY       US-WI  2021-12-27            328       5143       155\n",
       "\n",
       "[351936 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/kwzdyjg14cq25v2k2b2l6mt40000gn/T/ipykernel_6550/3710152624.py:14: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  all_to_node = edge_target_list.groupby([\"start_date\", \"Timeline\",  \"destination\"])[\"visitor_flows\", \"pop_flows\"].sum().reset_index()\n",
      "/var/folders/qt/kwzdyjg14cq25v2k2b2l6mt40000gn/T/ipykernel_6550/3710152624.py:29: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  node_to_all = edge_target_list.groupby([\"start_date\", \"Timeline\", \"origin\"])[\"visitor_flows\", \"pop_flows\"].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Add reverse flow value\n",
    "###############\n",
    "\n",
    "edge_target_list_reverse = edge_target_list[[\"start_date\", \"Timeline\",  \"visitor_flows\", \"pop_flows\", \"origin\", \"destination\"]].copy()\n",
    "edge_target_list_reverse.rename(columns = {\"visitor_flows\": \"visitor_flows_reverse\", \"pop_flows\": \"pop_flows_reverse\",\n",
    "                                           \"origin\":\"destination\", \"destination\":\"origin\"}, inplace=True)\n",
    "\n",
    "edge_target_list = pd.merge(edge_target_list, edge_target_list_reverse, on=[\"start_date\", \"Timeline\",  \"origin\", \"destination\"], how=\"left\")\n",
    "\n",
    "###############\n",
    "# All to node\n",
    "###############\n",
    "all_to_node = edge_target_list.groupby([\"start_date\", \"Timeline\",  \"destination\"])[\"visitor_flows\", \"pop_flows\"].sum().reset_index()\n",
    "\n",
    "all_to_node.rename(columns={\"visitor_flows\": \"all_visitor_flows_to_d\",\n",
    "                            \"pop_flows\": \"all_pop_flows_to_d\"}, inplace=True)\n",
    "\n",
    "edge_target_list = pd.merge(edge_target_list, all_to_node, on=[\"start_date\", \"Timeline\",  \"destination\"], how=\"left\")\n",
    "\n",
    "all_to_node.rename(columns={\"all_visitor_flows_to_d\": \"all_visitor_flows_to_o\",\n",
    "                            \"all_pop_flows_to_d\": \"all_pop_flows_to_o\", \"destination\":\"origin\"}, inplace=True)\n",
    "\n",
    "edge_target_list = pd.merge(edge_target_list, all_to_node, on=[\"start_date\", \"Timeline\", \"origin\"], how=\"left\")\n",
    "\n",
    "###############\n",
    "# Node to all\n",
    "###############\n",
    "node_to_all = edge_target_list.groupby([\"start_date\", \"Timeline\", \"origin\"])[\"visitor_flows\", \"pop_flows\"].sum().reset_index()\n",
    "\n",
    "node_to_all.rename(columns={\"visitor_flows\": \"o_visitor_flows_to_all\",\n",
    "                            \"pop_flows\": \"o_pop_flows_to_all\",}, inplace=True)\n",
    "\n",
    "edge_target_list = pd.merge(edge_target_list, node_to_all, on=[\"start_date\", \"Timeline\", \"origin\"], how=\"left\")\n",
    "\n",
    "node_to_all.rename(columns={\"o_visitor_flows_to_all\": \"d_visitor_flows_to_all\",\n",
    "                            \"o_pop_flows_to_all\": \"d_pop_flows_to_all\", \"origin\":\"destination\"}, inplace=True)\n",
    "\n",
    "edge_target_list = pd.merge(edge_target_list, node_to_all, on=[\"start_date\", \"Timeline\", \"destination\"], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Save to csv\n",
    "###############\n",
    "node_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/node_list.csv\", index=False)\n",
    "edge_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/edge_list.csv\", index=False)\n",
    "edge_target_list.to_csv(\"../Output_datasets/GeoDS_mobility_flow_prediction/edge_target_list.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node dataset\n",
    "node_id='iso_3166_2_code'\n",
    "node_timestamp=''\n",
    "node_features=['population_2019', 'population_density_2019','Total_Area',\n",
    "               'residential_land_use_area', 'commercial_land_use_area',\n",
    "               'industrial_land_use_area', 'retail_land_use_area', 'natural_land_use_area',\n",
    "\t           'residential_roads_length', 'other_roads_length', 'main_roads_length',\n",
    "\t\t       'point_transport', 'building_transport', 'point_food', 'building_food',\n",
    "\t           'point_health', 'building_health', 'point_education', 'building_education',\n",
    "               'point_retail', 'building_retail']\n",
    "\n",
    "# Edge dataset\n",
    "flow_origin='origin'\n",
    "flow_destination='destination'\n",
    "flows_value='pop_flows'\n",
    "flows_timestamp='Timeline'\n",
    "flows_features=['neighbouring', 'distances', 'visitor_flows', 'start_date',\n",
    "                'visitor_flows_reverse', 'pop_flows_reverse',\n",
    "                'all_visitor_flows_to_d', 'all_pop_flows_to_d',\n",
    "                'all_visitor_flows_to_o', 'all_pop_flows_to_o',\n",
    "                'o_visitor_flows_to_all', 'o_pop_flows_to_all',\n",
    "                'd_visitor_flows_to_all', 'd_pop_flows_to_all']\n",
    "\n",
    "# Chunk parameters\n",
    "chunk_size = 50\n",
    "\n",
    "# Chunked data path\n",
    "chunk_path  = \"../Output_datasets/GeoDS_mobility_flow_prediction/Chunked_merged_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge edge_list and edge_target_list\n",
    "edge_list = pd.merge(edge_list, edge_target_list, on=['origin', 'destination'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only neccesary columns\n",
    "nodes_columns = node_features + [node_id] + [node_timestamp]\n",
    "nodes_columns = [i for i in nodes_columns if i!='']\n",
    "node_list = node_list[nodes_columns]\n",
    "\n",
    "edges_columns = flows_features + [flow_origin] + [flow_destination] + \\\n",
    "    [flows_timestamp] + [flows_value]\n",
    "edges_columns = [i for i in edges_columns if i!='']\n",
    "edge_list = edge_list[edges_columns]\n",
    "\n",
    "# Merge nodes and edges\n",
    "node_list.rename(columns={node_id: flow_origin, node_timestamp: flows_timestamp}, inplace=True)\n",
    "node_list[flow_destination] = node_list[flow_origin]\n",
    "\n",
    "nodes_and_edges = pd.merge(pd.merge(edge_list, node_list.drop(flow_destination, axis=1), how='left', on=[flow_origin]),\n",
    "                        node_list.drop(flow_origin, axis=1), how='left', on=[flow_destination], \n",
    "                        suffixes=('_o', '_d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_period_list = [nodes_and_edges[flows_timestamp].unique()[i:i+chunk_size] for i in range(0, len(nodes_and_edges[flows_timestamp].unique())-(chunk_size-1)) if i%10==0]\n",
    "for chunk in chunk_period_list:\n",
    "    nodes_and_edges_chunk = nodes_and_edges[nodes_and_edges[flows_timestamp].isin(chunk)]\n",
    "    chunk_name = str(min(chunk)) + \"-\" + str(max(chunk))\n",
    "    nodes_and_edges_chunk.to_csv(f\"{chunk_path}/{chunk_name}.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d7d815e1f708ebb1f5714e36652b1af8a8532e2538a9a16a8fb7dc9a472af4e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
